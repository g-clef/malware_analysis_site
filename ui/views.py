import datetime
import hashlib
import json
import os
import zipfile

import lief
import pefile
import requests
import ssdeep
from PastebinDecoder import PastebinDecoder

from django.http import HttpResponse
from django.conf import settings
from rest_framework.response import Response
from rest_framework.exceptions import ParseError
from rest_framework.decorators import api_view, permission_classes, parser_classes
from rest_framework.parsers import JSONParser
from rest_framework import permissions

# Create your views here.


def make_auth_header():
    return {"Authorization": f"Token {settings.ARCHIVE_SERVER_TOKEN}"}


def index(_):
    return HttpResponse("hello")


@api_view(http_method_names=["GET", "POST"])
@permission_classes([permissions.IsAuthenticated, ])
@parser_classes([JSONParser, ])
def submit(request):
    # 1 check if entry for this zipfile already exists in archiver (previous processing may have failed)
    #   if so, use that entry for further work, otherwise make new entry in archiver for the zipfile
    # for each file in archive:
    #     2 check if a File entry for this file already exists in the archiver (again, processing may have failed)
    #        if so, use it for future work, otherwise make one here
    #     3 if metadata entry doesn't already exist for the file:
    #         get metadata for file (md5, sha1, sha256, ssdeep)
    #         submit metadata entry to archiver.
    #     4 if it's an executable:
    #         if header entry doesn't already exist for the file:
    #             get header information about it (ImpHash, CompileTime, Lief info)
    #             submit header entry to archiver
    #         if VT info doesn't already exist for file:
    #             Look it up in VirusTotal, get VT results
    #             submit VT info to entry to archiver.
    password = request.data.get("password")
    # NOTE: path should be the relative path from the base of ARCHIVE_PATH to the actual file.
    path = request.data.get("path")
    if path is None:
        raise ParseError("must specify path")
    origin_url = request.POST.get("origin_url")
    source = request.data.get("source")
    if source is None:
        raise ParseError("must specify source.")
    if password is not None and not isinstance(password, bytes):
        password = password.encode("utf-8")
    analyzer = PastebinDecoder.PasteDecoder()
    full_path = os.path.join(settings.ARCHIVE_PATH, path)
    filename = os.path.split(path)[-1]
    # Part 1
    headers = make_auth_header()
    # NOTE: Because of Django *stuff*, this path needs to be the full path on the Django server to the file
    # This is not the local path to the file, but the full path on the server filesystem. I feel like this is
    # a bit of a data leak, but I'm working inside django.
    # that means that the "path" variable should be the *relative* path from the ARCHIVE_PATH to the file,
    # and that ARCHIVE_PATH on this server and the archive site should be the same.
    exists_request = requests.get(settings.ARCHIVE_SERVER_URL + f"/archive/exists/?Path={full_path}",
                                  headers=headers).json()
    if exists_request["status"] is True:
        archive_id = exists_request["obj"]["id"]
    else:
        data = {"FileName": filename,
                "Path": full_path,
                "Source": source,
                "Origin_url": origin_url}
        create_request = requests.post(settings.ARCHIVE_SERVER_URL + "/archive/", json=data, headers=headers)
        if create_request.status_code not in [200, 201]:
            raise ParseError(f"Error creating archive. {create_request.content} {data}")
        result = create_request.json()
        archive_id = result['obj']["id"]

    filehandle = zipfile.ZipFile(full_path, mode="r")
    for member_name in filehandle.namelist():
        inner_filehandle = filehandle.open(member_name, pwd=password, mode="r")
        # this reads everything into RAM, which is a shame. Do this better later.
        inner_file_data = inner_filehandle.read()
        exists_request = requests.get(
            settings.ARCHIVE_SERVER_URL + f"/file/exists/?Archive={archive_id}&FileName={member_name}",
            headers=headers
        )
        exists_data = exists_request.json()
        # Part 2
        if exists_data["status"] is True:
            file_id = exists_data["obj"]["id"]
            # get all data about file from url here
            file_data = requests.get(settings.ARCHIVE_SERVER_URL + f"/file/{file_id}", headers=headers)
            file_type = file_data.json()["FileType"]
        else:
            # get encoding and file type here, create file
            file_type, file_data_list, encodings = analyzer.handle(inner_file_data)
            post_data = {
                "Archive": archive_id,
                "FileName": member_name,
                "FileType": file_type,
                "Encoding": encodings
            }
            create_request = requests.post(settings.ARCHIVE_SERVER_URL + "/file/", json=post_data, headers=headers)
            if create_request.status_code not in [200, 201]:
                raise ParseError(f"Error creating file entry: {create_request.content} {post_data}")
            file_id = create_request.json()['id']

        # Part 3
        exists_request = requests.get(settings.ARCHIVE_SERVER_URL + f"/metadata/exists/?FileID={file_id}", headers=headers)
        exists_data = exists_request.json()
        if exists_data["status"] is False:
            # get metadata about the file, submit here:
            sha1 = hashlib.sha1(inner_file_data)
            sha256 = hashlib.sha256(inner_file_data)
            md5 = hashlib.md5(inner_file_data)
            deep = ssdeep.hash(inner_file_data)
            post_data = {
                "File": file_id,
                "SHA256": sha256.hexdigest(),
                "MD5": md5.hexdigest(),
                "SHA1": sha1.hexdigest(),
                "SSDeep": deep
            }
            create_request = requests.post(settings.ARCHIVE_SERVER_URL + "/metadata/",
                                           json=post_data,
                                           headers=headers)
            if create_request.status_code not in [200, 201]:
                raise ParseError(f"Error creating metadata entry: {create_request.content} {post_data}")
        else:
            md5 = exists_data["obj"]['MD5']

        # part 4
        if file_type in settings.EXECUTABLE_TYPES:
            exists_request = requests.get(settings.ARCHIVE_SERVER_URL + f"/peinfo/exists/?FileID={file_id}", headers=headers)
            exists_data = exists_request.json()
            if exists_data["status"] is False:
                # get pe header info about the file, submit here:
                pe = pefile.PE(data=inner_file_data)
                imphash = pe.get_imphash()
                compile_time = pe.FILE_HEADER.TimeDateStamp
                if compile_time is not None:
                    compile_time = datetime.datetime.fromtimestamp(compile_time, tz=datetime.timezone.utc).isoformat()
                lief_obj = lief.parse(inner_file_data)
                # Lief params to care about taken from EMBER dataset:
                # https://github.com/elastic/ember/blob/9e5b60392c3799cba424a9e3a582acbf08faf378/ember/features.py#L292
                lief_info = {
                        'size': len(inner_file_data),
                        'vsize': lief_obj.virtual_size,
                        'has_debug': int(lief_obj.has_debug),
                        'exports': len(lief_obj.exported_functions),
                        'imports': len(lief_obj.imported_functions),
                        'has_relocations': int(lief_obj.has_relocations),
                        'has_resources': int(lief_obj.has_resources),
                        'has_signature': int(lief_obj.has_signature),
                        'has_tls': int(lief_obj.has_tls),
                        'symbols': len(lief_obj.symbols),
                        'coff': {
                            'timestamp': lief_obj.header.time_date_stamps,
                            'machine': str(lief_obj.header.machine).split('.')[-1],
                            'characteristics': [str(c).split('.')[-1] for c in lief_obj.header.characteristics_list]
                        },
                        "optional": {
                            'subsystem': str(lief_obj.optional_header.subsystem).split('.')[-1],
                            'dll_characteristics': [
                                str(c).split('.')[-1] for c in lief_obj.optional_header.dll_characteristics_lists
                            ],
                            'magic': str(lief_obj.optional_header.magic).split('.')[-1],
                            'major_image_version': lief_obj.optional_header.major_image_version,
                            'minor_image_version': lief_obj.optional_header.minor_image_version,
                            'major_linker_version': lief_obj.optional_header.major_linker_version,
                            'minor_linker_version': lief_obj.optional_header.minor_linker_version,
                            'major_operating_system_version': lief_obj.optional_header.major_operating_system_version,
                            'minor_operating_system_version': lief_obj.optional_header.minor_operating_system_version,
                            'major_subsystem_version': lief_obj.optional_header.major_subsystem_version,
                            'minor_subsystem_version': lief_obj.optional_header.minor_subsystem_version,
                            'sizeof_code': lief_obj.optional_header.sizeof_code,
                            'sizeof_headers': lief_obj.optional_header.sizeof_headers,
                            'sizeof_heap_commit': lief_obj.optional_header.sizeof_heap_commit
                        }
                    }
                data = {"File": file_id,
                        "ImpHash": imphash,
                        "CompileTime": compile_time,
                        "LIEFInfo": json.dumps(lief_info)}
                create_request = requests.post(settings.ARCHIVE_SERVER_URL + "/peinfo/", json=data, headers=headers)
                if create_request.status_code not in [200, 201]:
                    raise ParseError(f"Error creating peinfo data entry: {create_request.content} {data}")

            if settings.ENABLE_VIRUSTOTAL:
                exists_request = requests.get(settings.ARCHIVE_SERVER_URL + f"/vt/exists/?FileID={file_id}",
                                              headers=headers)
                exists_data = exists_request.json()
                if exists_data["status"] is False:
                    # get VirusTotal info about the file, submit here:
                    query_data = {"resource": md5, "apikey": settings.VT_API_KEY}
                    response = requests.post(settings.VT_URL, data=query_data)
                    if response.status_code == 204:
                        # when you exceed quota, they return blank json
                        pass
                    else:
                        info = response.json()
                        if "permalink" in info:
                            data = {"File": file_id,
                                    "Results": info}
                            create_request = requests.post(settings.ARCHIVE_SERVER_URL + "/vt/",
                                                           json=data,
                                                           headers=headers)
                            if create_request.status_code not in [200, 201]:
                                raise ParseError(f"Error creating vt data entry: {create_request.content} {data}")

    return Response({"status": "success"})
